# Story 3.1: Locally Analyze Webcam Image on Query

## Status: Approved

## Story

-   As an AI Enthusiast,
-   when I initiate a query (via voice or text),
-   I want Sourcer to locally analyze the current webcam image so that it can understand what it's looking at.

## Acceptance Criteria (ACs)

1.  When a query (transcribed voice from Story 2.1 or typed text) is received by the `CoreOrchestrator`, it successfully captures the current frame (`FrameData`) from the `VideoService`.
2.  The captured frame and the query text are passed to the `VisionService` for processing.
3.  The `VisionService` correctly orchestrates the local AI models (YOLO, then optionally MobileSAM based on LLaVA's needs, then LLaVA) to analyze the frame in context of the query.
4.  The `VisionService` generates a relevant textual description (`SceneDescription.full_text`) of the primary objects or scene content, focusing on the 10-15 common household/office objects identified in the PRD.
5.  The generated textual description is returned from `VisionService` to the `CoreOrchestrator` and is available for subsequent processing (UI display and TTS are covered in other stories).
6.  The entire visual analysis process, including all AI model inferences, runs entirely on the local machine without external network calls for AI processing.
7.  Basic error handling is implemented: if the vision pipeline fails (e.g., a model fails to load or inference crashes), the failure is caught, logged, and a generic error indication is passed back to the `CoreOrchestrator`.
8.  The AI model loading (YOLO, SAM, LLaVA) is managed efficiently (e.g., loaded once at startup or on first use) to minimize processing delays for each query.

## Tasks / Subtasks

-   [ ] **Task 1: Implement `VisionService` (AC: 2, 3, 4, 6, 8)**
    -   [ ] Create/Update `src/sourcer/services/vision_service.py`.
    -   [ ] Define a `VisionService` class.
    -   [ ] In its constructor (`__init__`):
        -   [ ] Load the local YOLO model (e.g., `YOLOModelWrapper`). Path and configuration from `config/settings.ini`.
        -   [ ] Load the local MobileSAM model (e.g., `MobileSAMModelWrapper`). Path and configuration from `config/settings.ini`.
        -   [ ] Load the local LLaVA model (e.g., `LLaVAModelWrapper`). Path and configuration from `config/settings.ini`.
        -   [ ] Implement singleton or managed loading for these models to ensure they are loaded only once.
    -   [ ] Implement the `analyze_frame(frame: FrameData, query_context: Optional[str] = None) -> str` method:
        -   [ ] Preprocess `frame` if needed for YOLO.
        -   [ ] Perform object detection using YOLO to get bounding boxes and labels.
        -   [ ] (Optional/Conditional) Based on YOLO output or `query_context`, decide if MobileSAM segmentation is needed to provide focus/context for LLaVA. If so, run MobileSAM.
        -   [ ] Prepare input for LLaVA (frame, query_context, optionally YOLO/SAM outputs).
        -   [ ] Generate textual description using LLaVA.
        -   [ ] Focus description generation on the 10-15 target objects.
        -   [ ] Post-process LLaVA output if necessary to get a clean `SceneDescription.full_text`.
        -   [ ] Return the textual description.
    -   [ ] Ensure all model execution within this service is local.
-   [ ] **Task 2: Implement AI Model Wrappers (AC: 3, 8)**
    -   [ ] If not already stubbed in Story 1.0, create wrapper classes (e.g., `YOLOModelWrapper`, `MobileSAMModelWrapper`, `LLaVAModelWrapper`) in `src/sourcer/services/` or a sub-module like `src/sourcer/services/ai_models/`.
    -   [ ] Each wrapper should handle:
        -   [ ] Loading its specific model from a path (provided via config).
        -   [ ] Providing a simple inference method (e.g., `detect(frame)`, `segment(frame, points)`, `describe(frame, text_prompt)`).
        -   [ ] Abstracting away the direct complexities of each model's library.
-   [ ] **Task 3: Update `CoreOrchestrator` to Use `VisionService` (AC: 1, 2, 5, 7)**
    -   [ ] In `src/sourcer/core/orchestrator.py`:
        -   [ ] Instantiate `VisionService`.
        -   [ ] Modify/Implement `process_query(query_text: str)` (or a similar method called after STT/text input):
            -   [ ] Call `VideoService.get_current_frame()` to obtain the current `FrameData`.
            -   [ ] If frame data is available, call `VisionService.analyze_frame(frame_data, query_text)`.
            -   [ ] Receive the textual description. For this story, store it or log it. (Actual UI/TTS handoff is in other stories).
            -   [ ] If `VideoService` provides no frame, handle gracefully (e.g., log, return "No video frame available" message).
            -   [ ] If `VisionService.analyze_frame` raises an exception or indicates an error, catch it, log it, and prepare a generic error message (e.g., "Visual analysis failed.").
-   [ ] **Task 4: Implement Threading for Vision Processing (AC: Architecture Adherence)**
    -   [ ] Ensure that the call to `VisionService.analyze_frame()` from the `CoreOrchestrator` (which might be triggered by a UI event or Pipecat event) is executed in a separate thread (e.g., `QThread` or `QtConcurrent` if `CoreOrchestrator` runs in main thread, or handled by `VisionService` internally) to prevent UI freezing during AI inference.
    -   [ ] Use signals/slots or appropriate callback mechanisms to return the result (description or error) to the `CoreOrchestrator` in a thread-safe manner.
-   [ ] **Task 5: Basic Error Handling and Logging (AC: 7)**
    -   [ ] Implement `try-except` blocks around model loading and inference calls in `VisionService` and its wrappers.
    -   [ ] Log errors using the `logging` module as per `docs/operational-guidelines.md`.
    -   [ ] Propagate a clear error status/message if analysis fails.

## Dev Technical Guidance

-   **Model Orchestration in `VisionService`:** The sequence of model calls (YOLO, optional SAM, LLaVA) needs careful implementation. Data (image, bounding boxes, masks) must be passed correctly between them.
-   **Model Loading:** AI models are large. Load them once (e.g., during `VisionService` initialization or on first use using a singleton pattern within wrappers) and keep them in memory to avoid long delays on each query. Refer to the Architecture Document's note on Singleton for Model Management.
-   **Configuration:** Model paths and any inference parameters should be configurable via `config/settings.ini` and loaded by `ConfigLoader`.
-   **Threading:** AI model inference can be CPU/GPU intensive and time-consuming. It's **critical** that these operations do **not** block the main UI thread or the Pipecat event loop. Use `QThread` for PyQt integration or ensure Pipecat/async patterns handle this if the orchestrator is part of that loop.
-   **Performance:** While detailed optimization is post-MVP, keep performance in mind (e.g., efficient frame handling, minimizing data conversions). The target is <3 seconds for voice round-trip, and vision analysis is a major part of this.
-   **Local Processing:** Double-check that all model libraries and inference calls are configured for local execution only.
-   **Data Types:** Ensure `FrameData` from `VideoService` is compatible with the input requirements of the `VisionService` (and underlying OpenCV/model expectations).

## Story Progress Notes

### Agent Model Used: `<Agent Model Name/Version>`

### Completion Notes List
{Any notes about implementation choices, difficulties, or follow-up needed}

### Change Log